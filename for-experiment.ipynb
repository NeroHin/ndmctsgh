{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-data-transformation\n",
    "\n",
    "```\n",
    "action：\n",
    "1. 將資料轉換為可以被機器學習的資料\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-25 16:58:59.746504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29618/2878644281.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# mian function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=========================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6276\u001b[0m         new_data = self._mgr.take(\n\u001b[0;32m-> 6277\u001b[0;31m             \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6278\u001b[0m         )\n\u001b[1;32m   6279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    868\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m             \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m         )\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m    686\u001b[0m                     ),\n\u001b[1;32m    687\u001b[0m                 )\n\u001b[0;32m--> 688\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m             ]\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    686\u001b[0m                     ),\n\u001b[1;32m    687\u001b[0m                 )\n\u001b[0;32m--> 688\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m             ]\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         new_values = algos.take_nd(\n\u001b[0;32m-> 1145\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         )\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import cudf as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, roc_curve, precision_recall_curve, average_precision_score, accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "df_path = 'dataset/train_with_ecg.csv'\n",
    "\n",
    "\n",
    "def pre_process(df):\n",
    "    # remove older label\n",
    "    df.drop(columns=['label'], inplace=True)\n",
    "    \n",
    "    # create new label\n",
    "    df['label'] = df['EF'].apply(lambda x: 1 if x <= 35 else 0)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def normalize(df):\n",
    "    # remove label, PID, UID, index\n",
    "    cols = df.drop(columns=['label', 'PID','UID', 'index']).columns\n",
    "    \n",
    "    # normalize data\n",
    "    for item in tqdm(cols):\n",
    "        mean_tmp = np.mean(np.array(df[item]))\n",
    "        std_tmp = np.std(np.array(df[item]))\n",
    "        if(std_tmp):\n",
    "            df[item] = df[item].apply(lambda x: (x - mean_tmp) / std_tmp)\n",
    "            \n",
    "    return df\n",
    "            \n",
    "\n",
    "def univariate_data(df, history_size, target_size):\n",
    "    target = df.values[:,-1]\n",
    "    df = df.drop(['label'],axis=1).values\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = history_size\n",
    "    end_index = len(df)\n",
    "    \n",
    "    # 多特徵\n",
    "    for i in tqdm(range(start_index, end_index)):\n",
    "\n",
    "        indices = range(i-history_size, i,10) # step表示滑动步长\n",
    "        data.append(df[indices])\n",
    "        labels.append(target[indices])\n",
    "        \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def buildManyToOneModel(shape, layer=1):\n",
    "\n",
    "    input_nodes = shape[1] * shape[2]\n",
    "    output_nodes = 1\n",
    "    hidden_nodes = int(round(2/3 * (input_nodes + output_nodes)))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    if layer == 1:\n",
    "        model.add(GRU(hidden_nodes, input_length=shape[1], input_dim=shape[2], kernel_initializer='normal'))\n",
    "        model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    PRC = tf.keras.metrics.AUC(curve='PR')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=[tf.keras.metrics.binary_accuracy,tf.keras.metrics.Recall(), PRC , tf.keras.metrics.AUC(curve='ROC')],run_eagerly=True)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# mian function\n",
    "\n",
    "df = pd.read_csv(df_path).sort_values(by=['PID', 'UID', 'index'])\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"start to pre-process data\")\n",
    "pre_process(df=df)\n",
    "# drop ef column\n",
    "df = df.drop(columns=['EF'])\n",
    "print(df.head())\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"start to normalize data\")\n",
    "normalize(df=df)\n",
    "print(df.head())\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"start to split data\")\n",
    "    # list of pid \n",
    "pid_list = df.PID.unique().tolist()\n",
    "\n",
    "# random shuffle pid list, select no repeat 0.6 from train, 0.2 from test, 0.2 from validation\n",
    "# ref: https://stackoverflow.com/questions/4211209/remove-all-the-elements-that-occur-in-one-list-from-another\n",
    "\n",
    "train_id = np.random.choice(pid_list, int(len(pid_list) * 0.6), replace=False).tolist()\n",
    "print(f\"train id list length: {len(train_id)}\")\n",
    "\n",
    "test_id  = np.random.choice([x for x in tqdm(pid_list) if x not in train_id], int(len(pid_list) * 0.2), replace=False).tolist()\n",
    "print(f\"test id list length: {len(test_id)}\")\n",
    "\n",
    "# sum of train and test id array\n",
    "train_test_id_list = train_id + test_id\n",
    "\n",
    "vali_id = np.random.choice([x for x in tqdm(pid_list) if x not in train_test_id_list], int(len(pid_list) * 0.2), replace=False).tolist()\n",
    "print(f\"test id list length: {len(test_id)}\")\n",
    "\n",
    "\n",
    "train_df = df[df['PID'].isin(train_id)].reset_index(drop=True).drop(['PID'],axis=1).sort_values(by=['UID', 'index']).drop(columns=['UID', 'index'])\n",
    "test_df = df[df['PID'].isin(test_id)].reset_index(drop=True).drop(['PID'],axis=1).sort_values(by=['UID', 'index']).drop(columns=['UID', 'index'])\n",
    "validation_df = df[df['PID'].isin(vali_id)].reset_index(drop=True).drop(['PID'],axis=1).sort_values(by=['UID', 'index']).drop(columns=['UID', 'index'])\n",
    "\n",
    "\n",
    "print(\"=========================================\")\n",
    "\n",
    "x_train_single, y_train_single = univariate_data(train_df,1000,0)\n",
    "x_val_single, y_val_single = univariate_data(validation_df,1000,0)\n",
    "x_test,y_test = univariate_data(test_df,1000,0)\n",
    "\n",
    "print(f'x_train_uni.shape -- {x_train_single.shape}')\n",
    "print(f'y_train_uni.shape -- {y_train_single.shape}')\n",
    "print(f'x_val_uni.shape -- {x_val_single.shape}')\n",
    "print(f'y_val_uni.shape -- {y_val_single.shape}')\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 150\n",
    "\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
    "train_data_single = train_data_single.cache().batch(BATCH_SIZE).shuffle(BUFFER_SIZE)#.repeat().shuffle(BUFFER_SIZE)\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "]\n",
    "\n",
    "\n",
    "buildManyToOneModel(shape=x_train_single.shape[-2:])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_data_single, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_data=val_data_single, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "Y_testPred = model.predict(X_test_shuffled)\n",
    "\n",
    "\n",
    "# AUC-ROC\n",
    "fpr, tpr, threshold = roc_curve(y_true=y_test, y_score=Y_testPred)\n",
    "# np.save(\n",
    "#     NNModel_folderpath + f'NN_Classifier_{classifier_name}_{layer}hiddenlayer-fprtpr-features_{feature_set}-target_{classify_label}-window_{input_window * 2}mins-sampling_{sampling_name}-round_{round_id}-{timestamp}.npy',\n",
    "#     np.array([fpr, tpr]))\n",
    "auc_score = auc(fpr, tpr)\n",
    "# AUC-ROC best threshold\n",
    "best_thre, best_tpr, best_fpr = opt_threshold(fpr, tpr, threshold, auc_score)\n",
    "# AUC-PRC & Average precision socre\n",
    "prc_precision, prc_recall, prc_threstholds = precision_recall_curve(y_true=y_test, probas_pred=Y_testPred)\n",
    "ap_score = average_precision_score(y_true=y_test, y_score=Y_testPred)\n",
    "\n",
    "Y_testPred = [1 if y >= best_thre else 0 for y in Y_testPred]\n",
    "cm = confusion_matrix(y_pred=Y_testPred, y_true=y_test)\n",
    "\n",
    "# metrics\n",
    "f1 = f1_score(y_pred=Y_testPred, y_true=y_test)\n",
    "sensitivity = cm[1][1] / (cm[1][1] + cm[1][0])  # TP/(TP+FN)\n",
    "specificity = cm[0][0] / (cm[0][0] + cm[0][1])  # TN/(TN+FP)\n",
    "precision_1 = cm[1][1] / (cm[1][1] + cm[0][1])  # TP/(TP+FP)\n",
    "recall_1 = cm[1][1] / (cm[1][1] + cm[1][0])  # TP/(TP+FN)\n",
    "accuracy = (cm[1][1] + cm[0][0]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1])  # (TP+TN)/(TP+FP+FN+TN)\n",
    "# model.save(NNModel_folderpath + f'NN_Classifier_{classifier_name}_{layer}hiddenlayer-features_{feature_set}-target_{classify_label}-window_{input_window * 2}mins-sampling_{sampling_name}-round_{round_id}-f1_{f1:.3f}-{timestamp}.h5')\n",
    "\n",
    "# delete model to release RAM\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "df_res = pd.DataFrame([[auc_score, best_thre, f1, sensitivity, specificity,\n",
    "                        precision_1, recall_1, accuracy, ap_score,cm[0][0], cm[0][1], cm[1][0], cm[1][1]]],\n",
    "                        columns=['AUC', 'best_threshold', 'f1_score', 'sensitivity', 'specificity',\n",
    "                                'precision_label1', 'recall_label1', 'accuracy', 'ap_score','TN', 'FP', 'FN', 'TP'])\n",
    "print(df_res.T)\n",
    "print(f'Confusion Matrix: \\n{cm}')\n",
    "print(f'Classification Report: \\n{classification_report(y_true=y_test, y_pred=Y_testPred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ec457515959363901d7ccb61b0e0e67fe0f5d7e34a074d892d26ce3aecbc0ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
